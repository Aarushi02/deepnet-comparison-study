# -*- coding: utf-8 -*-
"""a1_part_2_aarushij_singh72.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Icc_Fk2LRbX-5RIaCfjnOoDHC2iKxHhV

#VGG-16-and-ResNet-18-comparison-for-image-classification
In this project we implement and compare VGG-16 (Version C) and ResNet-18 for image classification. We also explore advanced techniques to improve model performance and the transition from standard deep CNNs to networks with residual connections.

## Step 1: Data preparation

1. Load CNN dataset.
"""

import os
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
from torch import nn, optim
import torchvision
from torchvision.utils import make_grid
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import zipfile
import random

def set_seed(seed=21):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed()

zip_path = 'cnn_part_2_dataset.zip'
extract_path = 'cnn_part_2_dataset'
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

base_dataset = datasets.ImageFolder(extract_path, transform=transform)

"""Analyze the dataset."""

class_names = base_dataset.classes
print(f"Classes: {class_names}")
print(f"Total images: {len(base_dataset)}")

"""2.	Create at least three different visualizations to explore the dataset."""

def show_sample_images(dataset, classes):
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for idx, cls in enumerate(classes):
        cls_idx = dataset.class_to_idx[cls]
        images = [img for img, label in dataset if label == cls_idx]
        axes[idx].imshow(np.transpose(images[0].numpy(), (1, 2, 0)))
        axes[idx].set_title(cls)
        axes[idx].axis('off')
    plt.show()

show_sample_images(base_dataset, base_dataset.classes)

"""The dataset contains 30000 images belonging to 3 different classes ('dogs', 'food', 'vehicles'). THe class distribution is balanced therefore, there is no need for sampling. Pixel Values stand between 0 to 1 (normalized)"""

def plot_class_distribution():
    counts = [0] * len(class_names)
    for _, label in base_dataset:
        counts[label] += 1
    plt.bar(class_names, counts)
    plt.title("Class Distribution")
    plt.xlabel("Class")
    plt.ylabel("Count")
    plt.show()

plot_class_distribution()

def plot_histogram():
    sample_img, _ = base_dataset[0]
    plt.hist(sample_img[0].numpy().flatten(), bins=50)
    plt.title("Pixel Value Distribution (Red Channel)")
    plt.show()
plot_histogram()

def compute_average_image(dataset, class_idx):
    images = [img.numpy() for img, label in dataset if label == class_idx]
    avg_img = np.mean(images, axis=0)
    return avg_img

fig, axes = plt.subplots(1, 3, figsize=(12, 4))
for idx, cls in enumerate(base_dataset.classes):
    avg_img = compute_average_image(base_dataset, idx)
    axes[idx].imshow(np.transpose(avg_img, (1, 2, 0)))
    axes[idx].set_title(f"Average {cls} Image")
    axes[idx].axis('off')
plt.show()

"""3. Split the dataset into training, testing, and validation sets."""

train_size = int(0.7 * len(base_dataset))
val_size = int(0.15 * len(base_dataset))
test_size = len(base_dataset) - train_size - val_size
train_set, val_set, test_set = random_split(base_dataset, [train_size, val_size, test_size])

def get_loaders(batch_size, train_set, val_set, test_set):
    return (
        DataLoader(train_set, batch_size=batch_size, shuffle=True),
        DataLoader(val_set, batch_size=batch_size, shuffle=False),
        DataLoader(test_set, batch_size=batch_size, shuffle=False)
    )

"""## Step 2: Implementing VGG

1. Implement the VGG-16 (Version C) architecture.
"""

class VGG16(nn.Module):
    def __init__(self, num_classes=3):
        super(VGG16, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU()
            # I have removed the final maxpool to accomodate our dataset
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 4 * 4, 4096), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), #VGG paper section 3.1 sets the droupout as 0.5
            nn.Linear(4096, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

"""2. Train your VGG-16 model."""

def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=10):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}
    epoch_times = []
    for epoch in range(num_epochs):
        for phase in ['train', 'val']:
            start_time = time.time()
            model.train() if phase == 'train' else model.eval()
            loader = dataloaders[phase]

            running_loss, running_corrects = 0.0, 0
            for inputs, labels in loader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(loader.dataset)
            epoch_acc = running_corrects.double() / len(loader.dataset)

            history[f"{phase}_loss"].append(epoch_loss)
            history[f"{phase}_acc"].append(epoch_acc.item())
            print(f"{phase} Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}")
            end_time = time.time()
            if phase == 'train':
              epoch_times.append(end_time - start_time)

        scheduler.step()

    avg_epoch_time = sum(epoch_times) / len(epoch_times)
    print(f"Average epoch time: {avg_epoch_time:.2f} seconds")
    return model, history

from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, dataloader):
    model.eval()
    device = next(model.parameters()).device
    running_loss = 0.0
    running_corrects = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    test_loss = running_loss / len(dataloader.dataset)
    test_acc = running_corrects.double() / len(dataloader.dataset)
    print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}")

    # Confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()

    # Classification report
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)
    print("\nPrecision, Recall, F1-score per class:")
    for i, class_name in enumerate(class_names):
        print(f"{class_name}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1={f1[i]:.2f}")

    return test_loss, test_acc.item(), all_preds, all_labels

"""We have experimented with weight initialization, optimizer, batch sizes and defined the models as follows:

| Model Name  | Weight Initialization | Optimizer | Batch Sizes| Test Accuracy | Per Epoch time |
|-------|-------|-------|-------|-------|-------|
| Model1 | Xavier  | Adam  | 64  | 33.78 % | 37.47 seconds |
| Model2  | He  | Adam  | 64 | 91.98% | 38.83 seconds  |
| Model3 | He  | SGD  | 256  | 87.42%   | 33.40 seconds |
| Model4 | He  | Adam  | 256  | 88.67%  | 33.56 seconds |

1. Weight Initialization : Xavier ; optimizer: Adam
"""

model1 = VGG16(num_classes=3)
criterion = nn.CrossEntropyLoss()
init_type = 'xavier'
if init_type == 'xavier':
    for m in model1.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model1.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')


optimizer1 = optim.Adam(model1.parameters(), lr=0.001)
scheduler1 = optim.lr_scheduler.StepLR(optimizer1, step_size=5, gamma=0.5)

batch_size = 64
train_loader, val_loader, test_loader = get_loaders(batch_size, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}

model1, history1 = train_model(model1, criterion, optimizer1, scheduler1, dataloaders, num_epochs=10)

test_loss, test_acc, all_preds, all_labels = evaluate_model(model1, test_loader)

import gc
del model1
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

"""2. Weight Initialization : he ; optimizer: Adam"""

model2 = VGG16(num_classes=3)
criterion = nn.CrossEntropyLoss()
init_type = 'he'
if init_type == 'xavier':
    for m in model2.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model2.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')

optimizer2 = optim.Adam(model2.parameters(), lr=0.001)
scheduler2 = optim.lr_scheduler.StepLR(optimizer2, step_size=5, gamma=0.5)

batch_size = 64
train_loader, val_loader, test_loader = get_loaders(batch_size, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}

model2, history2 = train_model(model2, criterion, optimizer2, scheduler2, dataloaders, num_epochs=10)

test_loss, test_acc, all_preds, all_labels = evaluate_model(model2, test_loader)

""" 3. Weight Initialization : he ; optimizer: SGD"""

model3 = VGG16(num_classes=3)
criterion = nn.CrossEntropyLoss()
init_type = 'he'
if init_type == 'xavier':
    for m in model3.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model3.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')

optimizer3 = optim.SGD(model3.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
scheduler3 = optim.lr_scheduler.StepLR(optimizer3, step_size=5, gamma=0.5)

batch_size_3 = 256
train_loader, val_loader, test_loader = get_loaders(batch_size_3, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}

model3, history3 = train_model(model3, criterion, optimizer3, scheduler3, dataloaders, num_epochs=10)

test_loss, test_acc, all_preds, all_labels = evaluate_model(model3, test_loader)

del model3
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

"""3. Weight Initialization : he ; optimizer: Adam ; Batch size = 256 (as specified in the vgg paper)"""

model4 = VGG16(num_classes=3)
criterion = nn.CrossEntropyLoss()

init_type = 'he'
if init_type == 'xavier':
    for m in model4.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model4.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')


optimizer4 = optim.Adam(model4.parameters(), lr=0.001)
scheduler4 = optim.lr_scheduler.StepLR(optimizer4, step_size=5, gamma=0.5)

batch_size_4 = 256
train_loader, val_loader, test_loader = get_loaders(batch_size_4, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}
model4, history4 = train_model(model4, criterion, optimizer4, scheduler4, dataloaders, num_epochs=10)

test_loss, test_acc, all_preds, all_labels = evaluate_model(model4, test_loader)

del model4
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

"""4. Apply regularization and overfitting prevention techniques."""

transform_aug = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

aug_dataset = datasets.ImageFolder(extract_path, transform=transform_aug)

train_size = int(0.7 * len(aug_dataset))
val_size = int(0.15 * len(aug_dataset))
test_size = len(aug_dataset) - train_size - val_size
train_set, val_set, test_set = random_split(aug_dataset, [train_size, val_size, test_size])

"""#### Improved VGG: Final"""

model2_reg = VGG16(num_classes=3)
criterion2_reg = nn.CrossEntropyLoss(label_smoothing=0.1)
init_type = 'he'
if init_type == 'xavier':
    for m in model2_reg.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model2_reg.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')

optimizer2_reg = optim.Adam(model2_reg.parameters(), lr=0.001, weight_decay=1e-4)
scheduler2_reg = optim.lr_scheduler.StepLR(optimizer2_reg, step_size=5, gamma=0.5)

batch_size = 64
train_loader, val_loader, test_loader = get_loaders(batch_size, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}

model2_reg, history2_reg = train_model(model2_reg, criterion2_reg, optimizer2_reg, scheduler2_reg, dataloaders, num_epochs=40)

criterion = nn.CrossEntropyLoss()
test_loss, test_acc, all_preds, all_labels = evaluate_model(model2_reg, test_loader)

"""5. Evaluation and analysis."""

def plot_training_curves(history):
    plt.figure(figsize=(12, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Val Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training vs Validation Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training vs Validation Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_training_curves(history2_reg)

"""Both the plots converge accurately"""

def show_misclassified(model, dataloader, num_images=8):
    model.eval()
    device = next(model.parameters()).device
    misclassified = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            for i in range(len(labels)):
                if preds[i] != labels[i]:
                    misclassified.append((inputs[i].cpu(), preds[i].cpu(), labels[i].cpu()))
                if len(misclassified) >= num_images:
                    break
            if len(misclassified) >= num_images:
                break

    plt.figure(figsize=(15, 5))
    for i, (img, pred, label) in enumerate(misclassified):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(img.permute(1, 2, 0), cmap="gray")
        plt.title(f"Pred: {class_names[pred]}\nTrue: {class_names[label]}")
        plt.axis('off')
    plt.suptitle("Misclassified Images")
    plt.show()

show_misclassified(model2, test_loader)

"""Model misclassified images that were actually hard to classify

6. Save the weights of the trained network.
"""

torch.save(model2_reg.state_dict(), 'a1_part2_vgg_aarushij_singh72.pth')

del model2
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

del model2_reg
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

"""## Step 3: Implementing ResNet

1. Implement residual blocks of ResNet.
"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        if self.downsample:
            identity = self.downsample(x)

        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += identity
        return self.relu(out)

"""2. Assemble the ResNet-18 architecture."""

class ResNet18(nn.Module):
    def __init__(self, num_classes=3):
        super(ResNet18, self).__init__()

        self.in_channels = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(64, 2)
        self.layer2 = self._make_layer(128, 2, stride=2)
        self.layer3 = self._make_layer(256, 2, stride=2)
        self.layer4 = self._make_layer(512, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

        layers = [ResidualBlock(self.in_channels, out_channels, stride, downsample)]
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        return self.fc(x)

"""3. Train the ResNet-18 model."""

def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=10):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

    for epoch in range(num_epochs):
        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            loader = dataloaders[phase]

            running_loss, running_corrects = 0.0, 0
            for inputs, labels in loader:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(loader.dataset)
            epoch_acc = running_corrects.double() / len(loader.dataset)

            history[f"{phase}_loss"].append(epoch_loss)
            history[f"{phase}_acc"].append(epoch_acc.item())
            print(f"{phase} Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}")

        scheduler.step()
    return model, history

def evaluate_model(model, test_loader):
    model.eval()
    device = next(model.parameters()).device
    y_true, y_pred = [], []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            y_true.extend(labels.cpu().tolist())
            y_pred.extend(preds.cpu().tolist())

    correct = sum(p == t for p, t in zip(y_pred, y_true))
    total = len(y_true)
    accuracy = correct / total * 100
    print(f"Test Accuracy: {accuracy:.2f}%")

    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)
    for i, cls in enumerate(class_names):
        print(f"{cls} â†’ Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}, F1: {f1[i]:.2f}")

model_resnet = ResNet18(num_classes=3)
init_type = 'he'
if init_type == 'xavier':
    for m in model_resnet.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model_resnet.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_resnet.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

train_loader, val_loader, test_loader = get_loaders(batch_size, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}
model_resnet, history = train_model(model_resnet, criterion, optimizer, scheduler, dataloaders, num_epochs=10)

evaluate_model(model_resnet, test_loader)

del model_resnet
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
set_seed()

"""4. Apply regularization techniques."""

model_resnet_reg = ResNet18(num_classes=3)
batch_size = 64
train_loader, val_loader, test_loader = get_loaders(batch_size, train_set, val_set, test_set)
dataloaders = {"train": train_loader, "val": val_loader}
criterion_2 = nn.CrossEntropyLoss(label_smoothing=0.1)
init_type = 'he'
if init_type == 'xavier':
    for m in model_resnet_reg.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
elif init_type == 'he':
    for m in model_resnet_reg.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')

optimizer_resnet_reg = optim.Adam(model_resnet_reg.parameters(), lr=0.001, weight_decay=1e-4)
scheduler_resnet_reg = optim.lr_scheduler.StepLR(optimizer_resnet_reg, step_size=5, gamma=0.5)
model_resnet_reg, history_resnet_reg = train_model(model_resnet_reg, criterion_2, optimizer_resnet_reg, scheduler_resnet_reg, dataloaders, num_epochs=40)

evaluate_model(model_resnet_reg, test_loader)

"""5. Evaluation and analysis."""

import matplotlib.pyplot as plt

# Extract values from training history
epochs = range(1, len(history_resnet_reg['train_loss']) + 1)

# Plot 1: Accuracy Comparison
plt.figure(figsize=(10, 6))
plt.plot(epochs, history_resnet_reg['train_acc'], 'b-o', label='ResNet-18 Train Acc')
plt.plot(epochs, history_resnet_reg['val_acc'], 'b--o', label='ResNet-18 Val Acc')
plt.plot(epochs, history2_reg['train_acc'], 'r-s', label='VGG-16 Train Acc')
plt.plot(epochs, history2_reg['val_acc'], 'r--s', label='VGG-16 Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy: ResNet-18 vs VGG-16')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot 2: Loss Comparison
plt.figure(figsize=(10, 6))
plt.plot(epochs, history_resnet_reg['train_loss'], 'b-o', label='ResNet-18 Train Loss')
plt.plot(epochs, history_resnet_reg['val_loss'], 'b--o', label='ResNet-18 Val Loss')
plt.plot(epochs, history2_reg['train_loss'], 'r-s', label='VGG-16 Train Loss')
plt.plot(epochs, history2_reg['val_loss'], 'r--s', label='VGG-16 Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss: ResNet-18 vs VGG-16')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""6. Save the weights of the trained network."""

torch.save(model_resnet_reg.state_dict(), 'a1_part2_resnet_aarushij_singh72.pth')

"""4. **Final Findings:**

*  Optimizers:

1.   SGD: SGD did not work well for VGG especially with the large batch size of 256
2.   Adam: Adam performed reasonally well.


*   Regularisation:

1.   Image augmentation: horizontal flip, rotation, and color jitter and normalization using the ImageNet mean and std
2.   weight deacy: 1e-4
3.  increased number of epochs from 10 to 40

These regularizations have performed well for the vgg model increasing the test accuracy

Our findings show that ResNet performs better than VGG in both accuracy and training time. Residual connections allow gradients to flow more easily through the network, mitigating the vanishing gradient problem and thus the model converges faster. This leads to improved performance and better generalization compared to VGG.

5. References.

* DL basics


  1.  https://docs.pytorch.org/tutorials/beginner/introyt/introyt_index.html
  2.  https://www.geeksforgeeks.org/building-a-convolutional-neural-network-using-pytorch/

*   VGG

  1.   VGG Paper https://arxiv.org/pdf/1409.1556
  2.   https://www.geeksforgeeks.org/vgg-16-cnn-model/


*   Pytorch Transforms
 https://docs.pytorch.org/vision/stable/transforms.html

* Scheduler
https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html

* Training with Pytoch
https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html

* Precision,Recall,Fscore
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html

* Weight Initialization

  1.   https://www.geeksforgeeks.org/initialize-weights-in-pytorch/
  2.   https://docs.pytorch.org/docs/stable/nn.init.html

* Optimization
https://docs.pytorch.org/docs/main/optim.html

* Regularization
  1.   Label Smoothing in CrossEntropyLoss https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html
  2.   Weight Decay https://docs.pytorch.org/docs/main/optim.html
  

* ResNet


  1.   ResNet Paper
  https://arxiv.org/pdf/1512.03385
  2.   https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/

* Google Colab's inbuilt AI assisstant Gemini was used for debugging.
"""